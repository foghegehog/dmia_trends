{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6hnVCrd9wXbg"
   },
   "source": [
    "# Capsule Networks HW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hWi3_oHdA02F"
   },
   "source": [
    "This homework is strongly based on https://github.com/higgsfield/Capsule-Network-Tutorial/blob/master/Capsule%20Network.ipynb. \n",
    "\n",
    "In case you want to check CapsNet on MNIST feel free to do that (link above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pWTldL4t9QYt"
   },
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XBmwmhZP9UDs"
   },
   "source": [
    "Your task is to **get at least 75% test accuracy on CIFAR10 (in the original paper ~90%) in no more than 10 epochs**. For this you may consider tuning various parameters.\n",
    "\n",
    "<br>\n",
    "Start with some **obvious stuff**:\n",
    "\n",
    "- Stack more convolutional layers before capsule layers.\n",
    "- Increase the size of the capsule layers (more capsules, larger capsules etc.). Note that it may take a lot of time.\n",
    "- Play with number of routing iterations in forward pass.\n",
    "- Play with kernel size of convolutions in the first layer (don't forget to change parameters of subsequent layers).\n",
    "- Play with kernel size of capsules in the second layer (again, pay attention to the parameters of subsequent computations).\n",
    "\n",
    "Then, you may consider more **advansed ideas**.\n",
    "\n",
    "- Try different variants of original implementation's loss function (change m+, m-, lambda, get rid of square etc.).\n",
    "- Try different loss functions (make it pure Hinge or pure MSE, maybe even cross-entropy!).\n",
    "- Try different implementation of capsules (not usual convolution operation, but maybe fully connected groups of neurons).\n",
    "- Try different non-linearities for capsules (changing ^2 to ^4 doesn't count!).\n",
    "- Try different weights for reconstruction loss.\n",
    "\n",
    "<br>\n",
    "For each of the tested ideas **supply a mini-report (create a markdown cell above the implementation)**. This mini-report should cover what is the idea (ex: _\"I increased the number of routing iterations from 3 to 10.\"_), and if the idea is not obvious (like stacking more layers), please supply 1/2-sentence justification for it (ex: _\"I changed m+ to 0.7 because I think that we really do not need such high certainty in classification. I speculate that it will speed-up training, taking into account that pictures in CIFAR10 are much less alike than in MNIST we really do not need to be 90% sure, just 70% and it will lead to the same result.\"_).\n",
    "\n",
    "<br>\n",
    "The final subtask is to **propose an idea in which capsules may be used in future and briefly explain motivation for it**. \n",
    "\n",
    "Ex: _\"The capsule representation of the object may be a great input for a GAN if coupled with usual GAN input (random noise vector). I think that capsules' generalized representations of objects may serve as an additional guide for a GAN so as to get expected results in image generation and not random ones. If we tune some coordinates in a capsule vector a little, we vary the reconstruction result (demonstrated in the paper), so, if we add up random noise vector and capsule representation (also tweaked to our desire) we can get various representations of the same object. For example we vary the coordinate 0 of the capsule and supply it to GAN we may get red apple, yellow apple, green apple etc. the random noise is needed to get different red apples, different yellow apples etc.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Mini-report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<small>The homework was performed by the user Prickly_u (registered under email kterehova@gmail.com)<br/></small>\n",
    "\n",
    "When launched on colab.research.google.com, initial implementation gave about 66% test accuracy. Introduced enhancements allowed to achieve 0.7563999999999997 accuracy on the 9th epoch (0.7555999999999998 on the 10th), with one epoch runing for ~833 seconds. (Output of the <b>Training</b> section was preserved; [jump to](#jump_training))<br/><br/>\n",
    "\n",
    "The modifications that helped to improve the result were as following:\n",
    "<ul>\n",
    "<li>In confirmation of the paper from the group chat, the most significant increase was gained by stacking additional convolutional layer in front of capsule layers. A new layer has changed the size of the input for subsequent ones, so the kernel sizes for the two convolutional layers and first capsule layer had to be aligned. The kernel_size=7 for convolution layers and kernel_size=5 for capsule layer were selected. As result, that gave about 4-5% growth. Appending one more convolutional layer showed no positive results.</li>\n",
    "<li><i>(parameters from \"advanced\" section).</i> The images in CIFAR-10 dataset are much more complex then in MNIST and the 32x32 resolution sometimes is not enough to discern similar classes even by the human eye. So, high threshold of m+ in the loss function can possibly lead to just overfitting on minor details. I've changed it to the 0.7 value, mentioned in this notebook. However, images from non-similar classes looks like having so many differences that it should be possible to say \"that's not ...\" with high degree of confidence. Thus, I've lowered the m- parameter to 0.05 value. Together, that improved the accuracy for about 1-2%. I've also tried to change the \"stability lambda\", but saw no growth. </li>\n",
    "<li><i>(parameters from \"advanced\" section).</i> During experiments with reconstruction_loss() function, I wondered what contribution it was giving and completely switched it off to see. Suddenly, that gave me remaining 3 - 3.5%. As the task was to improve recognition accuracy only, and reconstructions were rubbish all the same, I've left the loss function in this state. I guess, the reconstruction_loss deteriorates the recognition accuracy due to either bad recognition quality on CIFAR-10 or by changing the loss function to the form that is difficult to take the gradent from. For me, summing of two loss function with different natures and behaviors looks like a doubtful idea, especially when suspecting that standard library optimizers can be aimed on standard loss functions' appearance. <br/>\n",
    "If the reconstruction quality was included in the task, I would prefer to train decoder separately, with its own loss function, optimizer and parameters update.</li>\n",
    "</ul>\n",
    "Besides this, I've tried to change the u = u.view(x.size(0), 64 &ast; 8 &ast; 8, -1) line in PrimaryCaps to make the last dimension  divide the output by eight capsules, not the eight width pixels. (yeah, I've done it even after clarification in the group chat). It looked like giving 1-2% growth, but led to numerous memory erros, especially when combined with other modifications, so I brought it back.\n",
    "<br/>\n",
    "<br/>\n",
    "Just in case, the attempts that didn't worked for me:\n",
    "<ul><li>Increasing num_iterations to 4 or 5 turned out to be useless, or even detrimental (I do not remember exactly)</li>\n",
    "<li>Neither the change of capsules amount in PrimaryCaps (10 or 12) nor amount of their output channels (96) took  effect. The out_channels parameter set to 128, again, caused memory erros.</li>\n",
    "<li>Before eliminating reconstruction_loss totally, I've played with it a little. I guess, that not absolute values of the pixels form an image, but their relative brightness do. (The dark and bright versions of the same image are still perceived as the same). So MAPE loss function looked as tempting variant; but as I know, it is hard to differentiate and it is recommended to use MAE along with log() and exp() instead of it. I've written the corresponding code, but unfortunately, this thought turned out to be too abstruse and the training process got absolutely destabilized :(</li>\n",
    "<li>The kernel_size parameters gone through several changes too, along with stride=2, but showed no significant success.</li></ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capsules Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possibly, the relations between many primary capsules and the whole object along with the relation between the whole object and a specific part may be used for damaged parts detections. When most of the capsules with hight value of c_ij coefficients give co-directional vectors, but one high-c_ij capsule gives a vector with absolutely different direction, this can be an indicator of malformed detail.<br/> This may be applied not only for checking an images block, but as well to search breakages in audio records or to detec pronunciation and accent mistakes. Human speach seems to fit the concept of hierarchial capsules quite well: the sounds form (a finit number of admissible) syllables, syllables in a specific order form words, a sequence of words following grammar rules form sentences (interrogative, narrative, imperative, etc.) and topics. The problem of a large number of combinations and computational complexity arises, of course, but as computer powers are increasing and reuse of pre-trained models is gaining popularity, that difficulties might be overcome in the achievable future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWibD7rLA02L"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6QkLRp28woWk"
   },
   "source": [
    "After you have uploaded this notebook to Google Collaboratory you may start running it (if you have fast GPU, you can run it locally). \n",
    "\n",
    "First of all you need to install pytorch. The code below does that for you.\n",
    "\n",
    "Every 24 hours Google Collaboratory reboots. Make sure you either run operations that require less than 24 hours of computation or you log your progress so that you can avoid starting from scratch every time.\n",
    "\n",
    "The notebooks are stored in your root Google Drive folder and you can save other files there as well (requires some googling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "n1XrJ3IcE4DV"
   },
   "outputs": [],
   "source": [
    "# http://pytorch.org/\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "jFNhTBo-8S_F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from time import time\n",
    "\n",
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nQlfiY0w1Gn"
   },
   "source": [
    "## CIFAR-10 data loader/generator.\n",
    "\n",
    "Code below setups image generators from folder './data' (this folder is not saved and will be erased every 24 hours). \n",
    "\n",
    "Normalization values for CIFAR10 are taken from pytorch website (usual normalization values for the task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "OdImV-LJwuOL"
   },
   "outputs": [],
   "source": [
    "class Cifar10:\n",
    "    def __init__(self, batch_size):\n",
    "        dataset_transform = transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "                   ])\n",
    "\n",
    "        train_dataset = datasets.CIFAR10('./data', train=True, download=True, transform=dataset_transform)\n",
    "        test_dataset = datasets.CIFAR10('./data', train=False, download=True, transform=dataset_transform)\n",
    "        \n",
    "        self.train_loader  = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJry2wSsHUa8"
   },
   "source": [
    "## Network\n",
    "\n",
    "Recall the architecture of CapsNet. This tutorial walks you through building process of it. Note that actual values of parameters such as \"number of capsules\", \"number of filters in the first layer\" etc. are not taken from MNIST implementation in the original paper, but instead from CIFAR10 implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0Z5bAZezXuY"
   },
   "source": [
    "![alt text](https://cdn-images-1.medium.com/max/2000/1*uItEGzY1I9NK6hl1u4hPYg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NhW37DmbH-7N"
   },
   "source": [
    "### Pre-capsule layer\n",
    "\n",
    "This is a usual convolution layer that extracts basic features from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "N1afD72K8S_X"
   },
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=256, kernel_size=7):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels,\n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=1\n",
    "                             )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(self.conv(x))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tB21rTXeICUI"
   },
   "source": [
    "### First capsule layer (PrimaryCaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PiDlqFB94Cad"
   },
   "source": [
    "This is the second layer of the network and the first one which contains capsules (recall that capsules are just groups of neurons).\n",
    "\n",
    "The squash operation is the following one:\n",
    "\n",
    "\\begin{align}\n",
    "v_j & = \\frac{(\\|s_j\\|^2)}{(1 + \\|s_j\\|^2)} \\frac{s_j}{\\|s_j\\|}\\\\\n",
    "\\end{align}\n",
    "\n",
    "It takes a vector s_j as input, normalizes it to unit norm and then adds some non-linearity so that large vectors become close to 1 and small vectors close to zero. Recall that it is needed to enforce the property of v_j's norm being the probability (or certainty) that object is detected by the capsule v_j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "55zyRgn18S_c"
   },
   "outputs": [],
   "source": [
    "class PrimaryCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=8, in_channels=256, out_channels=64, kernel_size=5):\n",
    "        super(PrimaryCaps, self).__init__()\n",
    "\n",
    "        self.capsules = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=2, padding=0) \n",
    "                          for _ in range(num_capsules)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        u = [capsule(x) for capsule in self.capsules]\n",
    "        u = torch.stack(u, dim=1)\n",
    "        #print u.size()\n",
    "        u = u.view(x.size(0), 64 * 8 * 8, -1)\n",
    "        #u = u.view(x.size(0), 8, 64 * 8 * 8).transpose(1,2)\n",
    "        #print u.size()\n",
    "        return self.squash(u)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        #print input_tensor.data[0]\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        #print squared_norm\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mtw2-UyJIHQ4"
   },
   "source": [
    "### Second capsule layer (DigitCaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_EUmV1wa3-G1"
   },
   "source": [
    "This is the final layer of the network and the one that contains digit-capsules (or in case of CIFAR10 - class-capsules) which predict the class on the image.\n",
    "\n",
    "Below you may see the dynamic routing algorithm from the original paper under the forward section of the layer.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/2000/1*ukE9EQ6Yd6IPIu1cLJWSEQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Lew0uSA-8S_g"
   },
   "outputs": [],
   "source": [
    "class DigitCaps(nn.Module):\n",
    "    def __init__(self, num_capsules=10, num_routes=64 * 8 * 8, in_channels=8, out_channels=16):\n",
    "        super(DigitCaps, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_routes = num_routes\n",
    "        self.num_capsules = num_capsules\n",
    "\n",
    "        self.W = nn.Parameter(torch.randn(1, num_routes, num_capsules, out_channels, in_channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.stack([x] * self.num_capsules, dim=2).unsqueeze(4)\n",
    "\n",
    "        W = torch.cat([self.W] * batch_size, dim=0)\n",
    "        u_hat = torch.matmul(W, x)\n",
    "\n",
    "        b_ij = Variable(torch.zeros(1, self.num_routes, self.num_capsules, 1))\n",
    "        if USE_CUDA:\n",
    "            b_ij = b_ij.cuda()\n",
    "\n",
    "        num_iterations = 3\n",
    "        #num_iterations = 4\n",
    "        for iteration in range(num_iterations):\n",
    "            c_ij = F.softmax(b_ij)\n",
    "            c_ij = torch.cat([c_ij] * batch_size, dim=0).unsqueeze(4)\n",
    "\n",
    "            s_j = (c_ij * u_hat).sum(dim=1, keepdim=True)\n",
    "            v_j = self.squash(s_j)\n",
    "            \n",
    "            if iteration < num_iterations - 1:\n",
    "                a_ij = torch.matmul(u_hat.transpose(3, 4), torch.cat([v_j] * self.num_routes, dim=1))\n",
    "                b_ij = b_ij + a_ij.squeeze(4).mean(dim=0, keepdim=True)\n",
    "\n",
    "        return v_j.squeeze(1)\n",
    "    \n",
    "    def squash(self, input_tensor):\n",
    "        squared_norm = (input_tensor ** 2).sum(-1, keepdim=True)\n",
    "        output_tensor = squared_norm *  input_tensor / ((1. + squared_norm) * torch.sqrt(squared_norm))\n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwrZ5H7qIUWn"
   },
   "source": [
    "### Reconstruction part of network (decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3vjTImHJ5rQq"
   },
   "source": [
    "This is the second task for the network, namely, to reconstruct the image from the final class-capsules. \n",
    "\n",
    "This is a useful technique of regularization to prevent overfitting and also to enforce the property of capsules representing the 'instantiation parameters' of the object. In other words, final capsule should contain information about the class it predicts and that information (implicitly) may be: rotation angle, distortion, illumination etc.\n",
    "\n",
    "The reconstruction is done by a simple decoder (stack of fully-connected layers). Below is the picture for MNIST.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/deepblacksky/capsnet-tensorflow/master/images/recong.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Wr4UA8_k8S_l"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.reconstraction_layers = nn.Sequential(\n",
    "            nn.Linear(16 * 10, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024*3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, data):\n",
    "        classes = torch.sqrt((x ** 2).sum(2))\n",
    "        classes = F.softmax(classes)\n",
    "        \n",
    "        _, max_length_indices = classes.max(dim=1)\n",
    "        masked = Variable(torch.sparse.torch.eye(10))\n",
    "        if USE_CUDA:\n",
    "            masked = masked.cuda()\n",
    "        masked = masked.index_select(dim=0, index=F.Variable(max_length_indices.squeeze(1).data))\n",
    "        \n",
    "        reconstructions = self.reconstraction_layers((x * masked[:, :, None, None]).view(x.size(0), -1))\n",
    "        reconstructions = reconstructions.view(-1, 3, 32, 32)\n",
    "        \n",
    "        return reconstructions, masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWHxjIKjIaYg"
   },
   "source": [
    "### Full network (CapsNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYDG4nCl7l5Q"
   },
   "source": [
    "This is a final forward pass for the whole network. The only new part here is the custom loss from the original paper.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1493/1*y-bVFuiLReqSSdmdZ6wAmA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9maeKxss8S_p"
   },
   "outputs": [],
   "source": [
    "class CapsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CapsNet, self).__init__()\n",
    "        self.conv_layer = ConvLayer()\n",
    "        self.conv_layer2 = ConvLayer(in_channels=256, out_channels=256)\n",
    "        self.primary_capsules = PrimaryCaps()\n",
    "        self.digit_capsules = DigitCaps()\n",
    "        self.decoder = Decoder()\n",
    "        \n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        output = self.digit_capsules(self.primary_capsules(self.conv_layer2(self.conv_layer(data))))\n",
    "        reconstructions, masked = self.decoder(output, data)\n",
    "        return output, reconstructions, masked\n",
    "    \n",
    "    def loss(self, data, x, target, reconstructions):\n",
    "        return self.margin_loss(x, target) #+ self.reconstruction_loss(data, reconstructions)\n",
    "    \n",
    "    def margin_loss(self, x, labels, size_average=True):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        v_c = torch.sqrt((x**2).sum(dim=2, keepdim=True))\n",
    "\n",
    "        #left = (F.relu(0.9 - v_c)**2).view(batch_size, -1)\n",
    "        #right = (F.relu(v_c - 0.1)**2).view(batch_size, -1)\n",
    "\n",
    "        #loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "        \n",
    "        left = (F.relu(0.7 - v_c)**2).view(batch_size, -1)\n",
    "        right = (F.relu(v_c - 0.05)**2).view(batch_size, -1)\n",
    "\n",
    "        loss = labels * left + 0.5 * (1.0 - labels) * right\n",
    "        loss = loss.sum(dim=1).mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def reconstruction_loss(self, data, reconstructions):\n",
    "        loss = self.mse_loss(reconstructions.view(reconstructions.size(0), -1), data.view(reconstructions.size(0), -1))\n",
    "        #loss = torch.exp(self.l1_loss(\n",
    "        #    torch.log(reconstructions.view(reconstructions.size(0), -1)),\n",
    "        #    torch.log(data.view(reconstructions.size(0), -1))))\n",
    "        return loss * 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpK01eKY8JHS"
   },
   "source": [
    "Here the model is compiled with Adam optimizer with basic parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "4IZH_lMV8S_s"
   },
   "outputs": [],
   "source": [
    "capsule_net = CapsNet()\n",
    "if USE_CUDA:\n",
    "    capsule_net = capsule_net.cuda()\n",
    "optimizer = Adam(capsule_net.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PF6hxTxDHrsk"
   },
   "source": [
    "<a id='jump_training'></a>\n",
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zoBoVHvR8VeX"
   },
   "source": [
    "Note that one epoch takes a lot of time even on GPU, so don't rush and plan everything ahead and try to justify your ideas prior to implementing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2822,
     "output_extras": [
      {
       "item_id": 2
      },
      {
       "item_id": 4
      },
      {
       "item_id": 69
      },
      {
       "item_id": 114
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9003013,
     "status": "ok",
     "timestamp": 1521109799288,
     "user": {
      "displayName": "Kseniia Terekhova",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116431504126441391007"
     },
     "user_tz": -180
    },
    "id": "goSEK6q28S_y",
    "outputId": "c62db8b3-10c7-4e9e-b953-0d41cf05ba42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy [batch 0]: 0.17\n",
      "train accuracy [batch 100]: 0.31\n",
      "train accuracy [batch 200]: 0.4\n",
      "train accuracy [batch 300]: 0.47\n",
      "train accuracy [batch 400]: 0.44\n",
      "train accuracy [batch 499]: 0.54\n",
      "Total train loss 0.298099694043\n",
      "Total train accuracy 0.38224\n",
      "Total time for training an epoch: 832\n",
      "test accuracy [batch 0]: 0.45\n",
      "test accuracy [batch 25]: 0.48\n",
      "test accuracy [batch 50]: 0.51\n",
      "test accuracy [batch 75]: 0.43\n",
      "test accuracy [batch 99]: 0.58\n",
      "Total test loss 0.234083374888\n",
      "Total test accuracy 0.5108000000000001\n",
      "train accuracy [batch 0]: 0.61\n",
      "train accuracy [batch 100]: 0.47\n",
      "train accuracy [batch 200]: 0.56\n",
      "train accuracy [batch 300]: 0.52\n",
      "train accuracy [batch 400]: 0.57\n",
      "train accuracy [batch 499]: 0.57\n",
      "Total train loss 0.208724203259\n",
      "Total train accuracy 0.56348\n",
      "Total time for training an epoch: 830\n",
      "test accuracy [batch 0]: 0.64\n",
      "test accuracy [batch 25]: 0.53\n",
      "test accuracy [batch 50]: 0.7\n",
      "test accuracy [batch 75]: 0.62\n",
      "test accuracy [batch 99]: 0.66\n",
      "Total test loss 0.191502436399\n",
      "Total test accuracy 0.6031000000000002\n",
      "train accuracy [batch 0]: 0.59\n",
      "train accuracy [batch 100]: 0.59\n",
      "train accuracy [batch 200]: 0.67\n",
      "train accuracy [batch 300]: 0.68\n",
      "train accuracy [batch 400]: 0.64\n",
      "train accuracy [batch 499]: 0.67\n",
      "Total train loss 0.174369799733\n",
      "Total train accuracy 0.6445999999999992\n",
      "Total time for training an epoch: 831\n",
      "test accuracy [batch 0]: 0.67\n",
      "test accuracy [batch 25]: 0.69\n",
      "test accuracy [batch 50]: 0.71\n",
      "test accuracy [batch 75]: 0.67\n",
      "test accuracy [batch 99]: 0.65\n",
      "Total test loss 0.16884399429\n",
      "Total test accuracy 0.6514000000000002\n",
      "train accuracy [batch 0]: 0.68\n",
      "train accuracy [batch 100]: 0.68\n",
      "train accuracy [batch 200]: 0.72\n",
      "train accuracy [batch 300]: 0.7\n",
      "train accuracy [batch 400]: 0.68\n",
      "train accuracy [batch 499]: 0.73\n",
      "Total train loss 0.153253685758\n",
      "Total train accuracy 0.6921199999999992\n",
      "Total time for training an epoch: 832\n",
      "test accuracy [batch 0]: 0.65\n",
      "test accuracy [batch 25]: 0.65\n",
      "test accuracy [batch 50]: 0.59\n",
      "test accuracy [batch 75]: 0.68\n",
      "test accuracy [batch 99]: 0.7\n",
      "Total test loss 0.151126716286\n",
      "Total test accuracy 0.6865999999999999\n",
      "train accuracy [batch 0]: 0.75\n",
      "train accuracy [batch 100]: 0.78\n",
      "train accuracy [batch 200]: 0.66\n",
      "train accuracy [batch 300]: 0.72\n",
      "train accuracy [batch 400]: 0.79\n",
      "train accuracy [batch 499]: 0.68\n",
      "Total train loss 0.139468428165\n",
      "Total train accuracy 0.72432\n",
      "Total time for training an epoch: 832\n",
      "test accuracy [batch 0]: 0.72\n",
      "test accuracy [batch 25]: 0.66\n",
      "test accuracy [batch 50]: 0.68\n",
      "test accuracy [batch 75]: 0.71\n",
      "test accuracy [batch 99]: 0.66\n",
      "Total test loss 0.142234459519\n",
      "Total test accuracy 0.7114999999999998\n",
      "train accuracy [batch 0]: 0.76\n",
      "train accuracy [batch 100]: 0.73\n",
      "train accuracy [batch 200]: 0.81\n",
      "train accuracy [batch 300]: 0.75\n",
      "train accuracy [batch 400]: 0.83\n",
      "train accuracy [batch 499]: 0.71\n",
      "Total train loss 0.128455823496\n",
      "Total train accuracy 0.7513200000000008\n",
      "Total time for training an epoch: 832\n",
      "test accuracy [batch 0]: 0.72\n",
      "test accuracy [batch 25]: 0.76\n",
      "test accuracy [batch 50]: 0.69\n",
      "test accuracy [batch 75]: 0.73\n",
      "test accuracy [batch 99]: 0.71\n",
      "Total test loss 0.135819415599\n",
      "Total test accuracy 0.7252999999999996\n",
      "train accuracy [batch 0]: 0.71\n",
      "train accuracy [batch 100]: 0.75\n",
      "train accuracy [batch 200]: 0.78\n",
      "train accuracy [batch 300]: 0.77\n",
      "train accuracy [batch 400]: 0.75\n",
      "train accuracy [batch 499]: 0.73\n",
      "Total train loss 0.118880849525\n",
      "Total train accuracy 0.7738600000000002\n",
      "Total time for training an epoch: 832\n",
      "test accuracy [batch 0]: 0.73\n",
      "test accuracy [batch 25]: 0.8\n",
      "test accuracy [batch 50]: 0.77\n",
      "test accuracy [batch 75]: 0.74\n",
      "test accuracy [batch 99]: 0.81\n",
      "Total test loss 0.131086331904\n",
      "Total test accuracy 0.7376999999999999\n",
      "train accuracy [batch 0]: 0.79\n",
      "train accuracy [batch 100]: 0.82\n",
      "train accuracy [batch 200]: 0.72\n",
      "train accuracy [batch 300]: 0.75\n",
      "train accuracy [batch 400]: 0.76\n",
      "train accuracy [batch 499]: 0.84\n",
      "Total train loss 0.111825492918\n",
      "Total train accuracy 0.7905999999999996\n",
      "Total time for training an epoch: 832\n",
      "test accuracy [batch 0]: 0.71\n",
      "test accuracy [batch 25]: 0.84\n",
      "test accuracy [batch 50]: 0.82\n",
      "test accuracy [batch 75]: 0.77\n",
      "test accuracy [batch 99]: 0.73\n",
      "Total test loss 0.127529717162\n",
      "Total test accuracy 0.7486000000000004\n",
      "train accuracy [batch 0]: 0.85\n",
      "train accuracy [batch 100]: 0.83\n",
      "train accuracy [batch 200]: 0.71\n",
      "train accuracy [batch 300]: 0.79\n",
      "train accuracy [batch 400]: 0.89\n",
      "train accuracy [batch 499]: 0.76\n",
      "Total train loss 0.104171506375\n",
      "Total train accuracy 0.8099600000000006\n",
      "Total time for training an epoch: 833\n",
      "test accuracy [batch 0]: 0.76\n",
      "test accuracy [batch 25]: 0.74\n",
      "test accuracy [batch 50]: 0.78\n",
      "test accuracy [batch 75]: 0.78\n",
      "test accuracy [batch 99]: 0.82\n",
      "Total test loss 0.123032610491\n",
      "Total test accuracy 0.7563999999999997\n",
      "train accuracy [batch 0]: 0.8\n",
      "train accuracy [batch 100]: 0.8\n",
      "train accuracy [batch 200]: 0.8\n",
      "train accuracy [batch 300]: 0.79\n",
      "train accuracy [batch 400]: 0.8\n",
      "train accuracy [batch 499]: 0.86\n",
      "Total train loss 0.0982294224203\n",
      "Total train accuracy 0.8235399999999995\n",
      "Total time for training an epoch: 834\n",
      "test accuracy [batch 0]: 0.83\n",
      "test accuracy [batch 25]: 0.85\n",
      "test accuracy [batch 50]: 0.76\n",
      "test accuracy [batch 75]: 0.67\n",
      "test accuracy [batch 99]: 0.75\n",
      "Total test loss 0.122466164604\n",
      "Total test accuracy 0.7555999999999998\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "# dataset = Mnist(batch_size)\n",
    "dataset = Cifar10(batch_size)\n",
    "\n",
    "n_epochs = 10\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    ep_start = time()\n",
    "    capsule_net.train()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    for batch_id, (data, target) in enumerate(dataset.train_loader):\n",
    "        st = time()\n",
    "\n",
    "        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.data[0]\n",
    "        \n",
    "        tr_accuracy = sum(np.argmax(masked.data.cpu().numpy(), 1) == \n",
    "                          np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)\n",
    "        train_accuracy += tr_accuracy\n",
    "        if batch_id % 100 == 0 or batch_id == 499:\n",
    "            print \"train accuracy [batch {}]:\".format(batch_id), tr_accuracy\n",
    "        en = time()\n",
    "#         print 'Sec per batch', round(en-st, 2)\n",
    "    ep_end = time()\n",
    "    print 'Total train loss', train_loss / len(dataset.train_loader)\n",
    "    print 'Total train accuracy', train_accuracy / len(dataset.train_loader)\n",
    "    print 'Total time for training an epoch: {}'.format(int(ep_end - ep_start))\n",
    "        \n",
    "    capsule_net.eval()\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    for batch_id, (data, target) in enumerate(dataset.test_loader):\n",
    "\n",
    "        target = torch.sparse.torch.eye(10).index_select(dim=0, index=target)\n",
    "        data, target = Variable(data), Variable(target)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "\n",
    "        output, reconstructions, masked = capsule_net(data)\n",
    "        loss = capsule_net.loss(data, output, target, reconstructions)\n",
    "\n",
    "        test_loss += loss.data[0]\n",
    "        ts_accuracy = sum(np.argmax(masked.data.cpu().numpy(), 1) == \n",
    "                          np.argmax(target.data.cpu().numpy(), 1)) / float(batch_size)\n",
    "        test_accuracy += ts_accuracy\n",
    "        if batch_id % 25 == 0 or batch_id == 99:\n",
    "            print \"test accuracy [batch {}]:\".format(batch_id), ts_accuracy\n",
    "    \n",
    "    print 'Total test loss', test_loss / len(dataset.test_loader)\n",
    "    print 'Total test accuracy', test_accuracy / len(dataset.test_loader)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XaYfSSjpHyDf"
   },
   "source": [
    "## Reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MXG6YWW8w2k"
   },
   "source": [
    "Here you can view the reconstructions obtained by your CapsNet. Nothing special here, just fun to visualize them. Actually, for mnist the reconstructions are great, however for CIFAR10 they are rubbish (see the original paper for clues on that).\n",
    "\n",
    "Be careful when running reconstructions after `keyboard_interrupt`, because this may result in wrong input-target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "1UjRCmmI8S_7"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images_separately(images):\n",
    "    \"Plot the six CIFAR10 images separately.\"\n",
    "    fig = plt.figure()\n",
    "    for j in xrange(1, 7):\n",
    "        ax = fig.add_subplot(1, 6, j)\n",
    "        ax.matshow(images[j-1], cmap = matplotlib.cm.binary)\n",
    "        plt.xticks(np.array([]))\n",
    "        plt.yticks(np.array([]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "F5_f1El78S_9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f634f42e630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAABFCAYAAAB9nJwHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nil4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6343a9f908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images_separately(data[:6,0].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 95,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 781,
     "status": "ok",
     "timestamp": 1521060964815,
     "user": {
      "displayName": "Kseniia Terekhova",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "116431504126441391007"
     },
     "user_tz": -180
    },
    "id": "sqg4U_AJ8TAB",
    "outputId": "8b712b80-4dc6-43c3-e085-48b92038a432"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcwAAABOCAYAAACt1CbhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXA8nxE+9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3f65103b90>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_images_separately(reconstructions[:6,0].data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "B3IApzaZ8TAE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SANDCkeQfXOA"
   },
   "source": [
    "## Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvvqaMcJfZqB"
   },
   "source": [
    "The **deadline is March 16, 2018 20.00**\n",
    "\n",
    "75% test accuracy in 10 epochs gives you 7 points.\n",
    "\n",
    "Playing with at least 2 parameters in the advanced section gives you another 2 points in case you supplied mini-report with justification for it.\n",
    "\n",
    "Proposing not-so-obvious research idea will give you another 1 point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HiXkxhDNueVt"
   },
   "source": [
    "## Contacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Os4SO3Cukpe"
   },
   "source": [
    "Should you have any questions regarding the assignment contact Nikita Seleznev: http://telegram.me/stleznev\n",
    "\n",
    "However, please, avoid an avalanche of messages during the last hours before the deadline :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DBjxzFCQ9SxO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "CapsuleNetwork_HW.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
